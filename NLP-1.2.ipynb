{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTugas 1 NLP No. 2\\n@author 23520050\\n@reference https://www.kaggle.com/poigal/cnn-on-glove-word-embedding\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Tugas 1 NLP No. 2\n",
    "@author 23520050\n",
    "@reference https://www.kaggle.com/poigal/cnn-on-glove-word-embedding\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required imports.\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x92 in position 458: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._string_convert\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers._string_box_utf8\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x92 in position 458: invalid start byte",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-c8a05f1efa13>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtest_filename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Data_Test.csv\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mtrain_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_filename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_filename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    457\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 458\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    459\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    460\u001b[0m         \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1194\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1195\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"nrows\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1196\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1198\u001b[0m         \u001b[1;31m# May alter columns / col_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   2153\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2154\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2155\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2156\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2157\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._string_convert\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers._string_box_utf8\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x92 in position 458: invalid start byte"
     ]
    }
   ],
   "source": [
    "# Load and read files.\n",
    "\n",
    "train_filename = \"Data_Train.csv\"\n",
    "test_filename = \"Data_Test.csv\"\n",
    "\n",
    "train_df = pd.read_csv(train_filename)\n",
    "test_df = pd.read_csv(test_filename)\n",
    "\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STORY</th>\n",
       "      <th>SECTION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3126</th>\n",
       "      <td>34 billion dollars which included more than 70...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7406</th>\n",
       "      <td>The reviewers' reports of broken screens went ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5582</th>\n",
       "      <td>Actor Karen Gillan has opened up about James G...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882</th>\n",
       "      <td>The Left Front has nominated former Kolkata ma...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3483</th>\n",
       "      <td>We have spoken about a ?1 trillion irrigation ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5892</th>\n",
       "      <td>For example, using the DataCultr platform, fin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7515</th>\n",
       "      <td>Google�s flood forecasting system uses machine...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5383</th>\n",
       "      <td>She appeared in Bichir�s directorial debut Un ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5338</th>\n",
       "      <td>Tamil politics have long been associated with ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7486</th>\n",
       "      <td>Voters queue up at various polling stations du...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7628 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  STORY  SECTION\n",
       "3126  34 billion dollars which included more than 70...        2\n",
       "7406  The reviewers' reports of broken screens went ...        1\n",
       "5582  Actor Karen Gillan has opened up about James G...        2\n",
       "882   The Left Front has nominated former Kolkata ma...        0\n",
       "3483  We have spoken about a ?1 trillion irrigation ...        0\n",
       "...                                                 ...      ...\n",
       "5892  For example, using the DataCultr platform, fin...        1\n",
       "7515  Google�s flood forecasting system uses machine...        1\n",
       "5383  She appeared in Bichir�s directorial debut Un ...        2\n",
       "5338  Tamil politics have long been associated with ...        0\n",
       "7486  Voters queue up at various polling stations du...        0\n",
       "\n",
       "[7628 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shuffle train data\n",
    "train_df = train_df.sample(frac=1)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all data: 7628, train data: 7246, validation data: 382\n"
     ]
    }
   ],
   "source": [
    "training_fraction = 0.95\n",
    "validatation_fration = 1 - training_fraction\n",
    "\n",
    "train_data_len = len(train_df)\n",
    "training_data_len = int(train_data_len * 0.95)\n",
    "validation_data_len = train_data_len - training_data_len\n",
    "\n",
    "print(\"all data: {}, train data: {}, validation data: {}\".format(train_data_len, training_data_len, validation_data_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = train_df[0:training_data_len]\n",
    "validation_dataset = train_df[training_data_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STORY</th>\n",
       "      <th>SECTION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3126</th>\n",
       "      <td>34 billion dollars which included more than 70...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7406</th>\n",
       "      <td>The reviewers' reports of broken screens went ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5582</th>\n",
       "      <td>Actor Karen Gillan has opened up about James G...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882</th>\n",
       "      <td>The Left Front has nominated former Kolkata ma...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3483</th>\n",
       "      <td>We have spoken about a ?1 trillion irrigation ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5873</th>\n",
       "      <td>From how she loves eating cheese, to fetching ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>The patent case is part of a two-year series o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3188</th>\n",
       "      <td>A spokeswoman from Google declined to comment....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>908</th>\n",
       "      <td>Besides, the residual maturity for amortisatio...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3602</th>\n",
       "      <td>If you�re a Note 5 user, you might now have a ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7246 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  STORY  SECTION\n",
       "3126  34 billion dollars which included more than 70...        2\n",
       "7406  The reviewers' reports of broken screens went ...        1\n",
       "5582  Actor Karen Gillan has opened up about James G...        2\n",
       "882   The Left Front has nominated former Kolkata ma...        0\n",
       "3483  We have spoken about a ?1 trillion irrigation ...        0\n",
       "...                                                 ...      ...\n",
       "5873  From how she loves eating cheese, to fetching ...        2\n",
       "399   The patent case is part of a two-year series o...        1\n",
       "3188  A spokeswoman from Google declined to comment....        1\n",
       "908   Besides, the residual maturity for amortisatio...        3\n",
       "3602  If you�re a Note 5 user, you might now have a ...        1\n",
       "\n",
       "[7246 rows x 2 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STORY</th>\n",
       "      <th>SECTION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2794</th>\n",
       "      <td>MUMBAI: A barrage of bad news have taken the s...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2460</th>\n",
       "      <td>Besides, e-voting will be a valid option for D...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3102</th>\n",
       "      <td>For fitness enthusiasts, there are numerous ap...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2294</th>\n",
       "      <td>It�s a wrap #saandkiaankh ?? Thank you all for...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4421</th>\n",
       "      <td>�People were angry with Vasundhara Raje and he...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5892</th>\n",
       "      <td>For example, using the DataCultr platform, fin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7515</th>\n",
       "      <td>Google�s flood forecasting system uses machine...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5383</th>\n",
       "      <td>She appeared in Bichir�s directorial debut Un ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5338</th>\n",
       "      <td>Tamil politics have long been associated with ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7486</th>\n",
       "      <td>Voters queue up at various polling stations du...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>382 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  STORY  SECTION\n",
       "2794  MUMBAI: A barrage of bad news have taken the s...        3\n",
       "2460  Besides, e-voting will be a valid option for D...        3\n",
       "3102  For fitness enthusiasts, there are numerous ap...        1\n",
       "2294  It�s a wrap #saandkiaankh ?? Thank you all for...        2\n",
       "4421  �People were angry with Vasundhara Raje and he...        0\n",
       "...                                                 ...      ...\n",
       "5892  For example, using the DataCultr platform, fin...        1\n",
       "7515  Google�s flood forecasting system uses machine...        1\n",
       "5383  She appeared in Bichir�s directorial debut Un ...        2\n",
       "5338  Tamil politics have long been associated with ...        0\n",
       "7486  Voters queue up at various polling stations du...        0\n",
       "\n",
       "[382 rows x 2 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STORY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3126</th>\n",
       "      <td>34 billion dollars which included more than 70...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7406</th>\n",
       "      <td>The reviewers' reports of broken screens went ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5582</th>\n",
       "      <td>Actor Karen Gillan has opened up about James G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882</th>\n",
       "      <td>The Left Front has nominated former Kolkata ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3483</th>\n",
       "      <td>We have spoken about a ?1 trillion irrigation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5873</th>\n",
       "      <td>From how she loves eating cheese, to fetching ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>The patent case is part of a two-year series o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3188</th>\n",
       "      <td>A spokeswoman from Google declined to comment....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>908</th>\n",
       "      <td>Besides, the residual maturity for amortisatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3602</th>\n",
       "      <td>If you�re a Note 5 user, you might now have a ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7246 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  STORY\n",
       "3126  34 billion dollars which included more than 70...\n",
       "7406  The reviewers' reports of broken screens went ...\n",
       "5582  Actor Karen Gillan has opened up about James G...\n",
       "882   The Left Front has nominated former Kolkata ma...\n",
       "3483  We have spoken about a ?1 trillion irrigation ...\n",
       "...                                                 ...\n",
       "5873  From how she loves eating cheese, to fetching ...\n",
       "399   The patent case is part of a two-year series o...\n",
       "3188  A spokeswoman from Google declined to comment....\n",
       "908   Besides, the residual maturity for amortisatio...\n",
       "3602  If you�re a Note 5 user, you might now have a ...\n",
       "\n",
       "[7246 rows x 1 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_training_dataset = training_dataset.drop(\"SECTION\", axis=1)\n",
    "X_training_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3126    2\n",
       "7406    1\n",
       "5582    2\n",
       "882     0\n",
       "3483    0\n",
       "       ..\n",
       "5873    2\n",
       "399     1\n",
       "3188    1\n",
       "908     3\n",
       "3602    1\n",
       "Name: SECTION, Length: 7246, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_training_dataset = training_dataset[\"SECTION\"]\n",
    "Y_training_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load 6865 entries for training set, 381 for validation set, 382 for test set\n"
     ]
    }
   ],
   "source": [
    "# Load data return three datasets or dataframe : train_df, validation_df, and test_df\n",
    "# Set train_fraction and validation fraction in range of (0, 1)\n",
    "# Test fraction = 1 - train_frac - validation_frac\n",
    "PATH = \"Data_Train.csv\"\n",
    "def load_data(path, train_frac, validation_frac):\n",
    "    df = pd.read_csv(path, encoding=\"cp1252\")\n",
    "    len_df = len(df)\n",
    "    len_train = int(train_frac * len_df)\n",
    "    len_validation = int(validation_frac * len_df)\n",
    "    len_test = len_df - len_train - len_validation\n",
    "    train_df = df[0:len_train]\n",
    "    validation_df = df[len_train:len_train+len_validation]\n",
    "    test_df = df[(len_train+len_validation):]\n",
    "    return train_df,validation_df,test_df\n",
    "\n",
    "train_df,validation_df,test_df = load_data(PATH, 0.9, 0.05)\n",
    "print(\"Load {} entries for training set, {} for validation set, {} for test set\".format(len(train_df),\\\n",
    "                                                                                        len(validation_df),\\\n",
    "                                                                                        len(test_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return features (X) and label (Y) component of given dataset.\n",
    "def prep_dataset(df, y_label):\n",
    "    return df.drop(\"SECTION\", axis=1), df[\"SECTION\"]\n",
    "\n",
    "x_train, y_train = prep_dataset(train_df, \"SECTION\")\n",
    "x_valid, y_valid = prep_dataset(validation_df, \"SECTION\")\n",
    "x_test, y_test = prep_dataset(test_df, \"SECTION\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STORY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>But the most painful was the huge reversal in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How formidable is the opposition alliance amon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Most Asian currencies were trading lower today...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If you want to answer any question, click on ‘...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In global markets, gold prices edged up today ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6860</th>\n",
       "      <td>But will this also mark an end to Jeremy Renne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6861</th>\n",
       "      <td>On Checkout, people will be able to buy direct...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6862</th>\n",
       "      <td>India heads into its first fully-digital natio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6863</th>\n",
       "      <td>So, you can say it is a balance of good recall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6864</th>\n",
       "      <td>Here are some more photos of Srinish and Pearl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6865 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  STORY\n",
       "0     But the most painful was the huge reversal in ...\n",
       "1     How formidable is the opposition alliance amon...\n",
       "2     Most Asian currencies were trading lower today...\n",
       "3     If you want to answer any question, click on ‘...\n",
       "4     In global markets, gold prices edged up today ...\n",
       "...                                                 ...\n",
       "6860  But will this also mark an end to Jeremy Renne...\n",
       "6861  On Checkout, people will be able to buy direct...\n",
       "6862  India heads into its first fully-digital natio...\n",
       "6863  So, you can say it is a balance of good recall...\n",
       "6864  Here are some more photos of Srinish and Pearl...\n",
       "\n",
       "[6865 rows x 1 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STORY</th>\n",
       "      <th>SECTION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>But the most painful was the huge reversal in ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How formidable is the opposition alliance amon...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Most Asian currencies were trading lower today...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If you want to answer any question, click on ‘...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In global markets, gold prices edged up today ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6860</th>\n",
       "      <td>But will this also mark an end to Jeremy Renne...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6861</th>\n",
       "      <td>On Checkout, people will be able to buy direct...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6862</th>\n",
       "      <td>India heads into its first fully-digital natio...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6863</th>\n",
       "      <td>So, you can say it is a balance of good recall...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6864</th>\n",
       "      <td>Here are some more photos of Srinish and Pearl...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6865 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  STORY  SECTION\n",
       "0     But the most painful was the huge reversal in ...        3\n",
       "1     How formidable is the opposition alliance amon...        0\n",
       "2     Most Asian currencies were trading lower today...        3\n",
       "3     If you want to answer any question, click on ‘...        1\n",
       "4     In global markets, gold prices edged up today ...        3\n",
       "...                                                 ...      ...\n",
       "6860  But will this also mark an end to Jeremy Renne...        2\n",
       "6861  On Checkout, people will be able to buy direct...        1\n",
       "6862  India heads into its first fully-digital natio...        0\n",
       "6863  So, you can say it is a balance of good recall...        2\n",
       "6864  Here are some more photos of Srinish and Pearl...        2\n",
       "\n",
       "[6865 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# Load GloVe model\n",
    "GLOVE_PATH = \"glove.6B.50d.txt\"\n",
    "\n",
    "def load_word_vectors(path):\n",
    "    word_vectors = dict()\n",
    "    f = open(path, encoding=\"utf-8\")\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        word_numbers = values[1:]\n",
    "        word_vectors[word] = word_numbers\n",
    "    f.close()\n",
    "    print(\"load {} word vectors.\".format(len(word_vectors)))\n",
    "    return word_vectors\n",
    "\n",
    "word_vectors = load_word_vectors(GLOVE_PATH)\n",
    "# word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def load_split_data(path):\n",
    "    df = pd.read_csv(path, encoding=\"cp1252\")\n",
    "    X = df.iloc[:,0]\n",
    "    Y = df.iloc[:,1]\n",
    "\n",
    "    X_training, X_validation, Y_training, Y_validation = train_test_split(X, Y, test_size=0.1, random_state=123)\n",
    "    X_validation, X_test, Y_validation, Y_test = train_test_split(X_validation, Y_validation, test_size=0.5, random_state=123)\n",
    "    \n",
    "    return X_training, X_validation, X_test, Y_training, Y_validation, Y_test\n",
    "\n",
    "X_training, X_validation, X_test, Y_training, Y_validation, Y_test = load_split_data(PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1088    Bloomberg reported on 22 March that the treasu...\n",
       "1472    Facebook, WhatsApp and Twitter have overhauled...\n",
       "4010    The donors bought 1,459 electoral bonds of den...\n",
       "66      ”In another tweet, he wrote, “#AvengersEndgame...\n",
       "7489    New Delhi: We profile two enterprise tech star...\n",
       "                              ...                        \n",
       "4060    \"It's why we're seeing Indian artists connect ...\n",
       "1346    A similar battle is expected in the Muzaffarna...\n",
       "3454    Huawei Mate 20 lineup consists of a radial cam...\n",
       "7533    Achievements of PM Modi, the nation’s growth a...\n",
       "3582    At the back the Redmi Note 6 Pro sports the sa...\n",
       "Name: STORY, Length: 6865, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Implementation imports\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Flatten, LSTM, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Dropout, Activation, concatenate\n",
    "from keras.layers.core import Reshape, Flatten\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras import regularizers\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 37268 unique tokens in training data.\n",
      "Found 8019 unique tokens in validation data.\n"
     ]
    }
   ],
   "source": [
    "NUM_WORDS = 30000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=NUM_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'')\n",
    "tokenizer_train = Tokenizer(num_words=NUM_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'')\n",
    "tokenizer_valid = Tokenizer(num_words=NUM_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'')\n",
    "tokenizer_tests = Tokenizer(num_words=NUM_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'')\n",
    "\n",
    "# Get values from all dataframe\n",
    "tokenizer.fit_on_texts(pd.read_csv(PATH, encoding=\"cp1252\"))\n",
    "\n",
    "# Get values from training dataframe\n",
    "tokenizer_train.fit_on_texts(X_training)\n",
    "\n",
    "# Get values from validation dataframe\n",
    "tokenizer_valid.fit_on_texts(X_validation)\n",
    "\n",
    "# Get values from test dataframe\n",
    "tokenizer_tests.fit_on_texts(X_test)\n",
    "\n",
    "# Count unique tokens\n",
    "word_index = tokenizer.word_index\n",
    "word_index_train = tokenizer_train.word_index\n",
    "word_index_valid = tokenizer_valid.word_index\n",
    "print('Found {} unique tokens in training data.'.format(len(word_index_train)))\n",
    "print('Found {} unique tokens in validation data.'.format(len(word_index_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_train = tokenizer_train.texts_to_sequences(X_training)\n",
    "sequences_valid = tokenizer_valid.texts_to_sequences(X_validation)\n",
    "sequences_tests = tokenizer_tests.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of X train and X validation tensor: (6865, 100) (381, 100)\n",
      "shape of Y train and Y validation tensor: (6865, 4) (381, 4)\n",
      "shape of X tests and Y tests      tensor: (382, 100) (382, 4)\n"
     ]
    }
   ],
   "source": [
    "# sequences_train\n",
    "# sequences_valid\n",
    "# sequences_test\n",
    "X_train_padded = pad_sequences(sequences_train, maxlen=100)\n",
    "X_valid_padded = pad_sequences(sequences_valid, maxlen=X_train_padded.shape[1])\n",
    "\n",
    "#Y_train_array = np.asarray(Y_training)\n",
    "#Y_valid_array = np.asarray(Y_validation)\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "Y_train_array = to_categorical(Y_training, 4)\n",
    "Y_valid_array = to_categorical(Y_validation, 4)\n",
    "Y_tests_array = to_categorical(Y_test, 4)\n",
    "\n",
    "print(\"shape of X train and X validation tensor: {} {}\".format(X_train_padded.shape, X_valid_padded.shape))\n",
    "print(\"shape of Y train and Y validation tensor: {} {}\".format(Y_train_array.shape, Y_valid_array.shape))\n",
    "print(\"shape of X tests and Y tests      tensor: {} {}\".format(X_tests_padded.shape, Y_tests_array.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "word_vectors = load_word_vectors(GLOVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 50\n",
    "vocabulary_size=min(len(word_index_train)+1, NUM_WORDS)\n",
    "embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))\n",
    "\n",
    "for word,i in word_index_train.items():\n",
    "    if i>=NUM_WORDS:\n",
    "        continue\n",
    "    try:\n",
    "        embedding_vector = word_vectors[word]\n",
    "        embedding_matrix[i] = np.array(embedding_vector)\n",
    "    except KeyError:\n",
    "        vec = np.zeros(EMBEDDING_DIM)\n",
    "        embedding_matrix[i] = vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define embedding layer\n",
    "embedding_layer = Embedding(vocabulary_size,\n",
    "                           EMBEDDING_DIM,\n",
    "                           weights=[embedding_matrix],\n",
    "                           trainable=True)\n",
    "X_train_padded.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = X_train_padded.shape[1]\n",
    "filter_sizes = [3,4]\n",
    "num_filters = 100\n",
    "drop = 0.4\n",
    "\n",
    "inputs = Input(shape=(sequence_length,))\n",
    "embedding = embedding_layer(inputs)\n",
    "reshape = Reshape((sequence_length,EMBEDDING_DIM,1))(embedding)\n",
    "\n",
    "conv_0 = Conv2D(num_filters, (filter_sizes[0], EMBEDDING_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "conv_1 = Conv2D(num_filters, (filter_sizes[1], EMBEDDING_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "\n",
    "maxpool_0 = MaxPooling2D((sequence_length - filter_sizes[0] + 1, 1), strides=(1,1))(conv_0)\n",
    "maxpool_1 = MaxPooling2D((sequence_length - filter_sizes[1] + 1, 1), strides=(1,1))(conv_1)\n",
    "\n",
    "merged_tensor = concatenate([maxpool_0, maxpool_1], axis=1)\n",
    "flatten = Flatten()(merged_tensor)\n",
    "reshape = Reshape((2*num_filters,))(flatten)\n",
    "dropout = Dropout(drop)(flatten)\n",
    "conc = Dense(40)(dropout)\n",
    "output = Dense(units=4, activation='softmax',kernel_regularizer=regularizers.l2(0.01))(conc)\n",
    "\n",
    "# this creates a model that includes\n",
    "model = Model(inputs, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set optimizer and compile the model.\n",
    "\n",
    "optimizer = Adam(lr=1e-3)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "215/215 [==============================] - 4s 19ms/step - loss: 0.1651 - val_loss: 2.9232\n",
      "Epoch 2/20\n",
      "215/215 [==============================] - 4s 19ms/step - loss: 0.1517 - val_loss: 2.7138\n",
      "Epoch 3/20\n",
      "215/215 [==============================] - 4s 19ms/step - loss: 0.1343 - val_loss: 2.7984\n",
      "Epoch 4/20\n",
      "215/215 [==============================] - 4s 19ms/step - loss: 0.1244 - val_loss: 2.6414\n",
      "Epoch 5/20\n",
      "215/215 [==============================] - 4s 19ms/step - loss: 0.1154 - val_loss: 3.4037\n",
      "Epoch 6/20\n",
      "215/215 [==============================] - 4s 19ms/step - loss: 0.1133 - val_loss: 2.8376\n",
      "Epoch 7/20\n",
      "215/215 [==============================] - 4s 19ms/step - loss: 0.1034 - val_loss: 3.0141\n",
      "Epoch 8/20\n",
      "215/215 [==============================] - 4s 19ms/step - loss: 0.0983 - val_loss: 3.1138\n",
      "Epoch 9/20\n",
      "215/215 [==============================] - 4s 19ms/step - loss: 0.1001 - val_loss: 3.2085\n",
      "Epoch 10/20\n",
      "215/215 [==============================] - 4s 19ms/step - loss: 0.0861 - val_loss: 3.4324\n",
      "Epoch 11/20\n",
      "215/215 [==============================] - 4s 19ms/step - loss: 0.0805 - val_loss: 2.8396\n",
      "Epoch 12/20\n",
      "215/215 [==============================] - 4s 19ms/step - loss: 0.0827 - val_loss: 3.4657\n",
      "Epoch 13/20\n",
      "215/215 [==============================] - 4s 19ms/step - loss: 0.0774 - val_loss: 3.8910\n",
      "Epoch 14/20\n",
      "215/215 [==============================] - 4s 19ms/step - loss: 0.0839 - val_loss: 3.0554\n",
      "Epoch 15/20\n",
      "215/215 [==============================] - 4s 19ms/step - loss: 0.0760 - val_loss: 3.2703\n",
      "Epoch 16/20\n",
      "215/215 [==============================] - 4s 19ms/step - loss: 0.0667 - val_loss: 3.1979\n",
      "Epoch 17/20\n",
      "215/215 [==============================] - 4s 19ms/step - loss: 0.0601 - val_loss: 3.0599\n",
      "Epoch 18/20\n",
      "215/215 [==============================] - 4s 19ms/step - loss: 0.0576 - val_loss: 3.5659\n",
      "Epoch 19/20\n",
      "215/215 [==============================] - 4s 19ms/step - loss: 0.0753 - val_loss: 3.6261\n",
      "Epoch 20/20\n",
      "215/215 [==============================] - 4s 18ms/step - loss: 0.0621 - val_loss: 3.4894\n"
     ]
    }
   ],
   "source": [
    "# Fitting the model.\n",
    "\n",
    "callbacks = [EarlyStopping(monitor='val_loss')]\n",
    "hist_adam = model.fit(X_train_padded, Y_train_array, \n",
    "                      epochs=20, \n",
    "                      validation_data=(X_valid_padded, Y_valid_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test data\n",
    "tokenizer_test = Tokenizer(num_words=NUM_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'')\n",
    "sequences_test = tokenizer_test.texts_to_sequences(X_test)\n",
    "X_tests_padded = pad_sequences(sequences_test, maxlen=X_train_padded.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use model to predict training, validation, and test data\n",
    "pred_train = model.predict(X_train_padded)\n",
    "pred_valid = model.predict(X_valid_padded)\n",
    "pred_tests = model.predict(X_tests_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Collect scores\n",
    "categories = [\"0\", \"1\", \"2\", \"3\"]\n",
    "auc = np.zeros((3, 4))\n",
    "auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train AUC: 0.9999886011866533 , Val AUC: 0.5544855967078189 , Test AUC: 0.5\n",
      "1 Train AUC: 0.9999842165866056 , Val AUC: 0.5540048905588358 , Test AUC: 0.5004988262910799\n",
      "2 Train AUC: 1.0 , Val AUC: 0.61004243281471 , Test AUC: 0.5\n",
      "3 Train AUC: 0.9999841134263975 , Val AUC: 0.4756147540983607 , Test AUC: 0.5\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-81-3d47e627011a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Train AUC:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mauc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\", Val AUC:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mauc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\", Test AUC:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mauc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mavg_auc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mauc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Average Train AUC:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mavg_auc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\", Average Val AUC:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mavg_auc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\", Average Test AUC:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mavg_auc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\numpy\\core\\_methods.py\u001b[0m in \u001b[0;36m_mean\u001b[1;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[0mis_float16_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m     \u001b[0mrcount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_count_reduce_items\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    139\u001b[0m     \u001b[1;31m# Make this warning show up first\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mrcount\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\numpy\\core\\_methods.py\u001b[0m in \u001b[0;36m_count_reduce_items\u001b[1;34m(arr, axis)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[0mitems\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0max\u001b[0m \u001b[1;32min\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m         \u001b[0mitems\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mitems\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "for i,x in enumerate(categories):\n",
    "    auc = np.array([\n",
    "        metrics.roc_auc_score(Y_train_array[:,i], pred_train[:,i]),\n",
    "        metrics.roc_auc_score(Y_valid_array[:,i], pred_valid[:,i]),\n",
    "        metrics.roc_auc_score(Y_tests_array[:,i], pred_tests[:,i]),\n",
    "    ])\n",
    "    print(x,\"Train AUC:\",auc[0],\", Val AUC:\",auc[1],\", Test AUC:\",auc[2])\n",
    "    \n",
    "avg_auc = auc.mean(axis=1)\n",
    "print(\"Average Train AUC:\",avg_auc[0],\", Average Val AUC:\",avg_auc[1],\", Average Test AUC:\",avg_auc[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
